===============================================================================
% computeCost.m

temp_1 = X* theta;
diff = y - temp_1;
diff_1 = diff';
temp_2 = diff_1*diff;
J = temp_2/(2*m);



===============================================================================
% gradientDescent.m

for iter = 1:num_iters

    % Instructions: Perform a single gradient step on the parameter vector
    %               theta. 
    %
    % Hint: While debugging, it can be useful to print out the values
    %       of the cost function (computeCost) and gradient here.
    %

    theta_0 = theta(1);
    theta_1 = theta(2);
   
    diff = X*theta - y;
    
    temp_0 = theta_0 - alpha * sum(diff)/m;
    temp_1 = theta_1 - alpha/m * diff'*X(:,2);
    
    theta(1) = temp_0;
    theta(2) = temp_1;

    % Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);

end
=============================================================================
